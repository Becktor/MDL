{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\gan\\\\python\\\\losses\\\\python'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6460036e7c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_editor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrid_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Collapse TFGAN into a tiered namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meval\u001b[0m  \u001b[1;31m# pylint:disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Collapse `estimator` into a single namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgan_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\python\\gan_estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgan_estimator_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan_estimator_impl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\estimator\\python\\gan_estimator_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariables\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvariable_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuples\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfgan_tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfgan_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummaries\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfgan_summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariables\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvariables_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfgan_losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mslim_learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\gan\\python\\losses\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Collapse losses into a single namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_wargs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtuple_losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_hooks\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mpath_hook_for_FileFinder\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isdir\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We import the modules needed.\n",
    "import sys, os, datetime, time\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import utils.utils as utils\n",
    "from IPython.display import clear_output\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{0}\".format(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasionMnist():\n",
    "    mnisttrain = utils.load_mnist(\"../data/\",\"train\")\n",
    "    mnisttest = utils.load_mnist(\"../data/\",\"t10k\")\n",
    "    train_data = mnisttrain[0]\n",
    "    train_label = mnisttrain[1]\n",
    "    test_data = mnisttest[0]\n",
    "    test_label = mnisttest[1]\n",
    "    np.unique(train_label)\n",
    "    train_dataSet = utils.DataSet(train_data, train_label)\n",
    "    test_dataSet = utils.DataSet(test_data, test_label)\n",
    "    return train_dataSet, test_dataSet\n",
    "\n",
    "def regMnist():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "    # Reshape to image for visualization\n",
    "    train_data = mnist.train.images\n",
    "    train_label = mnist.train.labels\n",
    "    test_data = mnist.test.images\n",
    "    test_label = mnist.test.labels\n",
    "    train_dataSet = utils.DataSet(train_data, train_label)\n",
    "    test_dataSet = utils.DataSet(test_data, test_label)\n",
    "    return train_dataSet, test_dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model using wrappers\n",
    "# the model is identical\n",
    "def discriminator(x, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        net = tf.layers.conv2d(x, 64, 4, padding=\"same\", activation = utils.lrelu)\n",
    "        net = tf.layers.batch_normalization(net, training = is_training)\n",
    "        net = tf.layers.conv2d(net, 128, 4, padding=\"same\", activation = utils.lrelu)\n",
    "        net = tf.layers.batch_normalization(net, training = is_training)\n",
    "        net = tf.reshape(net, [-1, 128*28*28])\n",
    "        net = tf.layers.dense(net, 1024, activation=utils.lrelu)\n",
    "        out_logit = tf.layers.dense(net, 1)\n",
    "        out = tf.nn.sigmoid(out_logit)\n",
    "    return out, out_logit, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create model using wrappers\n",
    "# the model is identical\n",
    "def generator(z, is_training=True, reuse=False):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    with tf.variable_scope(\"generator\", reuse = reuse):\n",
    "        net = tf.layers.dense(z, 1024, activation = tf.nn.relu)\n",
    "        net = tf.layers.batch_normalization(net, training = is_training)\n",
    "        net = tf.layers.dense(net, 128 * 7 * 7, activation = tf.nn.relu)\n",
    "        net = tf.layers.batch_normalization(net, training = is_training)\n",
    "        net = tf.reshape(net, [-1, 7, 7, 128])\n",
    "        net = tf.layers.conv2d_transpose(net, 64, 4, strides = 2, padding = \"same\", activation = tf.nn.relu)\n",
    "        net = tf.layers.batch_normalization(net, training = is_training)\n",
    "        out = tf.layers.conv2d_transpose(net, 1, 4, strides = 2, padding = \"same\", activation = utils.lrelu)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generator(x):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        fc1=tf.layers.dense(x, 128 * 10 * 10, activation = tf.nn.elu)\n",
    "        \n",
    "        bn_1 = tf.layers.batch_normalization(fc1)\n",
    "        reshape = tf.reshape(bn_1, [-1, 10, 10, 128]) # bwhc\n",
    "        print(reshape)\n",
    "        dconv_1 = tf.layers.conv2d_transpose(reshape, 128, 5, strides = 2, padding = \"valid\", activation=tf.nn.elu)\n",
    "        print(dconv_1)\n",
    "        bn_2 = tf.layers.batch_normalization(dconv_1)\n",
    "        conv_2 = tf.layers.conv2d(bn_2, 128, 7, padding=\"same\", activation = tf.nn.elu)\n",
    "        print(conv_2)\n",
    "        bn_3 = tf.layers.batch_normalization(conv_2)\n",
    "        dconv_2 = tf.layers.conv2d_transpose(bn_3, 64, 5, strides = 2, padding = \"same\", activation=tf.nn.elu)\n",
    "        print(dconv_2)\n",
    "        bn_4 = tf.layers.batch_normalization(dconv_2)\n",
    "        out = tf.layers.conv2d(bn_4, 1, 7, padding=\"valid\", activation=tf.nn.elu)     \n",
    "        print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create model using wrappers\n",
    "# the model is identical\n",
    "def generator(x):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        fcn1 = tf.layers.dense(x, 128, activation=tf.nn.relu)\n",
    "        out = tf.layers.dense(fcn1, 784, activation=tf.nn.sigmoid)\n",
    "    return out\n",
    "\n",
    "def discriminator(x, reuse = False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse = reuse):\n",
    "        # 1st conv-layer block\n",
    "        fcn1 = tf.layers.dense(x, 128, activation=tf.nn.relu)\n",
    "        D_prob = tf.layers.dense(fcn1, 1, activation=None)\n",
    "    return D_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the neural net\n",
    "# step size for gradient decsent\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "#Iterations of forward and back pass\n",
    "training_iters = 2**24\n",
    "\n",
    "#size of each batch used for forward/backwards pass\n",
    "batch_size = 128\n",
    "display_step = 256\n",
    "Z_dim = 100\n",
    "d_steps = 3\n",
    "train_dataSet, test_dataSet = fasionMnist()\n",
    "tf.reset_default_graph()\n",
    "# tf Graph input\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "    y = tf.placeholder(tf.int64, None)\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, 100], name='Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "G_sample = generator(Z)\n",
    "D_real, D_real_logits, _ = discriminator(x)\n",
    "D_fake, D_fake_logits, _ = discriminator(G_sample, reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    #find mean loss of batch\n",
    "    #D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "    #G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "    # Alternative loss functions\n",
    "    #D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.zeros_like(D_logit_real)))\n",
    "    #D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "    #D_loss = D_loss_real + D_loss_fake\n",
    "    #G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "    #Wasserstein\n",
    "    #D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)\n",
    "    #G_loss = -tf.reduce_mean(D_fake)\n",
    "    d_loss_real = - tf.reduce_mean(D_real_logits)\n",
    "    d_loss_fake = tf.reduce_mean(D_fake_logits)\n",
    "    D_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # get loss for generator\n",
    "    G_loss = - d_loss_fake\n",
    "    \n",
    "theta_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"discriminator\")\n",
    "theta_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"generator\")\n",
    "    \n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_min = tf.train.AdamOptimizer(lr, beta1=beta1).minimize(D_loss, var_list=theta_D)\n",
    "    G_min = tf.train.AdamOptimizer(lr * 3, beta1=beta1).minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "                \n",
    "clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D]\n",
    "\n",
    "# Add scalars to Tensorboard\n",
    "\"\"\" Summary \"\"\"\n",
    "d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "d_loss_sum = tf.summary.scalar(\"d_loss\", D_loss)\n",
    "g_loss_sum = tf.summary.scalar(\"g_loss\", G_loss)\n",
    "#tf.contrib.layers.summarize_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
    "d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name for tensorboard.\n",
    "name = 'tb_'+ str(datetime.datetime.now().strftime('%Y-%m-%d_%H%M_%S'))\n",
    "cwd = os.getcwd()\n",
    "tb_path = os.path.join(cwd, \"Tensorboard\")\n",
    "tb_path = os.path.join(tb_path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(tb_path, sess.graph)\n",
    "disc_output = [D_min, clip_D, D_loss, d_sum]\n",
    "gen_output = [G_min, G_loss, g_sum]\n",
    "start_time = time.time()\n",
    "try:\n",
    "    stepd = 1\n",
    "    stepg = 1\n",
    "    step = 1\n",
    "    # Keep training until max iterations is reached\n",
    "    while step * batch_size < training_iters:\n",
    "        \n",
    "        # load first batch\n",
    "        for _ in range(d_steps):\n",
    "            X_mb, _ = train_dataSet.next_batch(mb_size)\n",
    "            feed_dict_disc = {x: X_mb, Z: utils.sample_Z(mb_size, Z_dim)}\n",
    "            _, _, D_loss_curr, summaryD = sess.run(disc_output, feed_dict=feed_dict_disc)\n",
    "            if stepd % display_step == 0:\n",
    "                writer.add_summary(summaryD, stepd)\n",
    "            stepd+=1\n",
    "            \n",
    "        X_mb, _ = train_dataSet.next_batch(mb_size)\n",
    "        feed_dict_gen = {x: X_mb, Z: utils.sample_Z(mb_size, Z_dim)}\n",
    "        _, G_loss_curr, summaryG = sess.run(gen_output, feed_dict=feed_dict_gen)\n",
    "        if stepg % display_step == 0:\n",
    "            writer.add_summary(summaryG, stepg)\n",
    "        stepg+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Testing step see if data is converging\n",
    "        if step % display_step == 0:\n",
    "            clear_output()\n",
    "            prec = ((step * batch_size)/training_iters)*100\n",
    "            print(\"Currently: \"+ str(prec)+\"%\")\n",
    "            print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\"\\\n",
    "                  % (step, training_iters, batch_size, time.time() - start_time, D_loss_curr, G_loss_curr))\n",
    "            \n",
    "            samples = sess.run(G_sample, feed_dict={Z: utils.sample_Z(16, Z_dim)})\n",
    "            fig = utils.plot(samples)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "            fig2 = utils.plot(X_mb[:16])\n",
    "            plt.show()\n",
    "            plt.close(fig2)\n",
    "        step+=1\n",
    "    print(\"\\nOptimization Finished!, Training GLOSS = {:.3f}\".format(G_loss_curr))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_D(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "samples = sess.run(G_sample, feed_dict={Z: sample_D(1, Z_dim)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = utils.plot(samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
